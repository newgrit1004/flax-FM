{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from flaxfm.utils import config, time_measure\n",
    "from flaxfm.dataset.movielens import MovieLens20MDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MovieLens20MDataset(dataset_path='/dist/dataset/ratings.csv')\n",
    "\n",
    "train_length = int(len(dataset) * 0.8)\n",
    "valid_length = int(len(dataset) * 0.1)\n",
    "test_length = len(dataset) - train_length - valid_length\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, (train_length, valid_length, test_length))\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=4)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=config.batch_size, num_workers=4)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=config.batch_size, num_workers=4)\n",
    "\n",
    "data_loader_dict = {}\n",
    "data_loader_dict['train'] = train_data_loader\n",
    "data_loader_dict['valid'] = valid_data_loader\n",
    "data_loader_dict['test'] = test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62501\n"
     ]
    }
   ],
   "source": [
    "print(len(data_loader_dict['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch FactorizationMachineModel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class FeaturesLinear(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.compat.long)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        d = x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return torch.sum(self.fc(x), dim=1) + self.bias\n",
    "\n",
    "\n",
    "class FeaturesEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim=16):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.compat.long)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return self.embedding(x)\n",
    "\n",
    "class FactorizationMachine(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, reduce_sum=True):\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "        sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix\n",
    "\n",
    "\n",
    "class FactorizationMachineModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Factorization Machine.\n",
    "    Reference:\n",
    "        S Rendle, Factorization Machines, 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        self.fm = FactorizationMachine(reduce_sum=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = self.linear(x) + self.fm(self.embedding(x))\n",
    "        return torch.sigmoid(x.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "net = FactorizationMachineModel(dataset.field_dims, 16)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import flax\n",
    "\n",
    "@time_measure\n",
    "def torch_train_function(data_loader_dict:Dict[str, torch.utils.data.DataLoader], config:flax.struct.dataclass):\n",
    "    for epoch in range(1, config.epochs+1):\n",
    "        running_loss, epoch_loss = 0.0, 0.0\n",
    "        for i, data in enumerate(data_loader_dict['train']):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            if i%2000 == 1999:\n",
    "                print(f'epoch {epoch}, {i+1} loss: {running_loss/2000}')\n",
    "                running_loss = 0\n",
    "\n",
    "        epoch_loss = epoch_loss / len(data_loader_dict['train'])\n",
    "\n",
    "        # Print the loss for each epoch\n",
    "        print(f'Epoch: {epoch}, Epoch Loss: {epoch_loss}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_train_function({'train': <torch.utils.data.dataloader.DataLoader object at 0x7fd2ed4ce730>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7fd21d306250>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7fd297e789a0>}, Config(seed=42, epochs=10, learning_rate=0.001, batch_size=256)) started at 14:08:16\n",
      "epoch 1, 2000 loss: 0.831310775756836\n",
      "epoch 1, 4000 loss: 0.7635619808733464\n",
      "epoch 1, 6000 loss: 0.6995131441652774\n",
      "epoch 1, 8000 loss: 0.6475981541275978\n",
      "epoch 1, 10000 loss: 0.6133821322023869\n",
      "epoch 1, 12000 loss: 0.5914396651983261\n",
      "epoch 1, 14000 loss: 0.576055371105671\n",
      "epoch 1, 16000 loss: 0.5664981813728809\n",
      "epoch 1, 18000 loss: 0.5591714956313372\n",
      "epoch 1, 20000 loss: 0.555097324743867\n",
      "epoch 1, 22000 loss: 0.5504828713089228\n",
      "epoch 1, 24000 loss: 0.5450075949132442\n",
      "epoch 1, 26000 loss: 0.5420327550023795\n",
      "epoch 1, 28000 loss: 0.53953737847507\n",
      "epoch 1, 30000 loss: 0.5386036107838154\n",
      "epoch 1, 32000 loss: 0.535680391073227\n",
      "epoch 1, 34000 loss: 0.5338230924010277\n",
      "epoch 1, 36000 loss: 0.5332352897077799\n",
      "epoch 1, 38000 loss: 0.5311131589561701\n",
      "epoch 1, 40000 loss: 0.530550006493926\n",
      "epoch 1, 42000 loss: 0.5291834669262171\n",
      "epoch 1, 44000 loss: 0.5279721605777741\n",
      "epoch 1, 46000 loss: 0.527243042498827\n",
      "epoch 1, 48000 loss: 0.5263853980898857\n",
      "epoch 1, 50000 loss: 0.5263431432247162\n",
      "epoch 1, 52000 loss: 0.5227779650092125\n",
      "epoch 1, 54000 loss: 0.5236516529172659\n",
      "epoch 1, 56000 loss: 0.5226679356843233\n",
      "epoch 1, 58000 loss: 0.5209501699060202\n",
      "epoch 1, 60000 loss: 0.5215561542659998\n",
      "epoch 1, 62000 loss: 0.520397577792406\n",
      "Epoch: 1, Epoch Loss: 0.5658296592608332\n",
      "torch_train_function({'train': <torch.utils.data.dataloader.DataLoader object at 0x7fd2ed4ce730>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7fd21d306250>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7fd297e789a0>}, Config(seed=42, epochs=10, learning_rate=0.001, batch_size=256)) finish at 14:37:51, total:1776.3695695400238 sec(s)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pytorch FactorizationMachineModel Train Result\n",
    "total GPU Usage : 21GB\n",
    "learning_rate : 0.001\n",
    "\n",
    "-epoch 1\n",
    "training time : 1776 sec = 약 30분.\n",
    "epoch loss : 0.5658296592608332\n",
    "\n",
    "Note:\n",
    "    This pytorch code did not use any efficient training method such as DP, DDP.\n",
    "    This pytorch code is general pytorch training code.\n",
    "    이 파이토치 코드는 효과적으로 트레이닝할 수 있는 방법(DP, DDP)을 적용하지 않았습니다.\n",
    "    이 파이토치 코드는 일반적으로 사용되는 파이토치 트레이닝 코드 양식을 따릅니다.\n",
    "\"\"\"\n",
    "torch_train_function(data_loader_dict, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flax FactorizationMachineModel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import flax\n",
    "import jax\n",
    "from flaxfm.layer import FeaturesLinearFlax, FeaturesEmbeddingFlax, FactorizationMachineFlax\n",
    "from flax import linen as nn\n",
    "from jaxlib.xla_extension import DeviceArray\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from optax._src.loss import sigmoid_binary_cross_entropy\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "def create_train_state(model:nn.Module, rng:DeviceArray,\n",
    "                        data_loader:torch.utils.data.dataloader.DataLoader):\n",
    "\n",
    "    params = model.init(rng, next(train_data_loader.__iter__())[0].numpy())['params']\n",
    "    optimizer = optax.adam(config.learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_model(state:train_state.TrainState, grads:flax.core.frozen_dict.FrozenDict):\n",
    "    return state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_epoch(state: train_state.TrainState, x_train:np.ndarray, y_train:np.ndarray):\n",
    "    def loss_fn(params):\n",
    "        preds = state.apply_fn({'params': params}, x_train)\n",
    "        loss = jnp.mean(sigmoid_binary_cross_entropy(preds, y_train))\n",
    "        return loss, preds\n",
    "\n",
    "    preds = state.apply_fn({'params': state.params}, x_train)\n",
    "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, _), grads = gradient_fn(state.params)\n",
    "    state = update_model(state, grads)\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "@time_measure\n",
    "def train_and_evaluate(data_loader_dict:Dict[str, torch.utils.data.dataloader.DataLoader],\n",
    "                        model:nn.Module):\n",
    "    rng = jax.random.PRNGKey(config.seed)\n",
    "    rng, init_rng = jax.random.split(rng)\n",
    "    state = create_train_state(model, init_rng, data_loader_dict['train'])\n",
    "\n",
    "    #train\n",
    "    for epoch in range(1, config.epochs+1):\n",
    "        running_loss, epoch_loss = [], []\n",
    "        for idx, batch in enumerate(data_loader_dict['train']):\n",
    "            x_train, y_train = list(map(lambda x : x.numpy(), batch))\n",
    "            state, loss = train_epoch(state, x_train, y_train)\n",
    "            epoch_loss.append(loss)\n",
    "            running_loss.append(loss)\n",
    "\n",
    "            if idx%2000 == 1999:\n",
    "                \"\"\"\n",
    "                많은 양의 epoch를 돌릴 경우 print문 주석처리\n",
    "                \"\"\"\n",
    "                print(f'epoch {epoch}, {idx+1} loss: {jnp.mean(np.array(running_loss))}')\n",
    "                running_loss = []\n",
    "        print(f'Epoch: {epoch}, Epoch Loss: {jnp.mean(np.array(epoch_loss))}')\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_and_evaluate({'train': <torch.utils.data.dataloader.DataLoader object at 0x7fd2ed4ce730>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7fd21d306250>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7fd297e789a0>}, FactorizationMachineModelFlax(\n",
      "    # attributes\n",
      "    field_dims = array([138493, 131262])\n",
      "    embed_dim = 16\n",
      ")) started at 14:02:17\n",
      "epoch 1, 2000 loss: 0.6728949546813965\n",
      "epoch 1, 4000 loss: 0.6630457043647766\n",
      "epoch 1, 6000 loss: 0.6508450508117676\n",
      "epoch 1, 8000 loss: 0.6383839845657349\n",
      "epoch 1, 10000 loss: 0.6270895600318909\n",
      "epoch 1, 12000 loss: 0.6193199753761292\n",
      "epoch 1, 14000 loss: 0.6123647093772888\n",
      "epoch 1, 16000 loss: 0.6084650754928589\n",
      "epoch 1, 18000 loss: 0.6043536067008972\n",
      "epoch 1, 20000 loss: 0.6018789410591125\n",
      "epoch 1, 22000 loss: 0.5997350811958313\n",
      "epoch 1, 24000 loss: 0.5975313186645508\n",
      "epoch 1, 26000 loss: 0.5963043570518494\n",
      "epoch 1, 28000 loss: 0.5950296521186829\n",
      "epoch 1, 30000 loss: 0.5943940877914429\n",
      "epoch 1, 32000 loss: 0.5929510593414307\n",
      "epoch 1, 34000 loss: 0.593069314956665\n",
      "epoch 1, 36000 loss: 0.5918009281158447\n",
      "epoch 1, 38000 loss: 0.5908004641532898\n",
      "epoch 1, 40000 loss: 0.5914736986160278\n",
      "epoch 1, 42000 loss: 0.5902916789054871\n",
      "epoch 1, 44000 loss: 0.5901402831077576\n",
      "epoch 1, 46000 loss: 0.5896667838096619\n",
      "epoch 1, 48000 loss: 0.5898377895355225\n",
      "epoch 1, 50000 loss: 0.5898157954216003\n",
      "epoch 1, 52000 loss: 0.5896586179733276\n",
      "epoch 1, 54000 loss: 0.5898712277412415\n",
      "epoch 1, 56000 loss: 0.5891557335853577\n",
      "epoch 1, 58000 loss: 0.5883120894432068\n",
      "epoch 1, 60000 loss: 0.5890825390815735\n",
      "epoch 1, 62000 loss: 0.5888930559158325\n",
      "Epoch: 1, Epoch Loss: 0.6039530634880066\n",
      "train_and_evaluate({'train': <torch.utils.data.dataloader.DataLoader object at 0x7fd2ed4ce730>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7fd21d306250>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7fd297e789a0>}, FactorizationMachineModelFlax(\n",
      "    # attributes\n",
      "    field_dims = array([138493, 131262])\n",
      "    embed_dim = 16\n",
      ")) finish at 14:03:40, total:83.9746961593628 sec(s)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Flax FactorizationMachineModel Train Result\n",
    "\n",
    "total GPU Usage : 21GB\n",
    "learning_rate : 0.001\n",
    "\n",
    "-epoch 10\n",
    "training time : 84 sec\n",
    "epoch loss : 0.6039863228797913\n",
    "\"\"\"\n",
    "from flaxfm.model.fm import FactorizationMachineModelFlax\n",
    "model = FactorizationMachineModelFlax(dataset.field_dims, 16)\n",
    "train_and_evaluate(data_loader_dict, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비교 결론\n",
    "\n",
    "- Training speed\n",
    "    - flax가 pytorch에 비해 훨씬 빠르다\n",
    "    - 같은 데이터에 대해서 flax는 10epoch에 84초, pytorch는 1epoch에 1776초 이므로 210배의 속도 차이가 난다고 볼 수 있음\n",
    "\n",
    "    \n",
    "- loss function 수렴도\n",
    "    - flax 기반 코드는 트레이닝 속도가 빠른 것은 확인되었지만, loss 수렴 속도는 pytorch 코드에 비해 낮은 모습을 보임\n",
    "    - 비교 사전에 기대했던 점은 동일한 epoch만큼 동일한 하이퍼파라미터 및 layer 구조로 트레이닝했을 때 flax와 pytorch가 비슷한 수준의 loss 값을 갖고 flax 코드의 시간 단축이 훨씬 빠른 점을 보고 싶었음\n",
    "    - 하지만, flax는 동일한 시간에 더 많은 epoch를 돌려볼 수 있으므로 특정 하이퍼파라미터에 따른 epoch loss를 pytorch에 비해 빠르게 관측하는 것이 가능\n",
    "    - flax의 loss function은 느리게 수렴하기 때문에 overfitting을 피할 수 있는 순간을 디테일하게 결정할 수 있을 것 같음\n",
    "    - 동일한 시간으로 비교하여 테스트하는 코드를 돌려보는 것이 더 바람직하다는 생각"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
