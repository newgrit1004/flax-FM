<!DOCTYPE html>
<head>
<meta charset="UTF-8">
<style>
.r1 {color: #005f00; text-decoration-color: #005f00}
.r2 {font-weight: bold}
.r3 {color: #000080; text-decoration-color: #000080; font-weight: bold}
.r4 {color: #87af00; text-decoration-color: #87af00; font-weight: bold}
.r5 {color: #005f00; text-decoration-color: #005f00; font-weight: bold}
.r6 {color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold}
.r7 {color: #000080; text-decoration-color: #000080; font-weight: bold; font-style: italic}
.r8 {color: #87af00; text-decoration-color: #87af00; font-weight: bold; font-style: italic}
.r9 {color: #005f00; text-decoration-color: #005f00; font-weight: bold; font-style: italic}
.r10 {color: #7f7f7f; text-decoration-color: #7f7f7f}
.r11 {color: #800000; text-decoration-color: #800000; font-weight: bold}
.r12 {color: #000080; text-decoration-color: #000080}
.r13 {color: #87af00; text-decoration-color: #87af00}
.r14 {color: #008000; text-decoration-color: #008000; background-color: #f8f8f8; font-weight: bold}
.r15 {color: #000000; text-decoration-color: #000000; background-color: #f8f8f8}
.r16 {color: #0000ff; text-decoration-color: #0000ff; background-color: #f8f8f8; font-weight: bold}
.r17 {background-color: #f8f8f8}
.r18 {color: #666666; text-decoration-color: #666666; background-color: #f8f8f8}
.r19 {color: #ba2121; text-decoration-color: #ba2121; background-color: #f8f8f8; font-style: italic}
.r20 {color: #008000; text-decoration-color: #008000; background-color: #f8f8f8}
.r21 {color: #0000ff; text-decoration-color: #0000ff; background-color: #f8f8f8}
.r22 {color: #aa22ff; text-decoration-color: #aa22ff; background-color: #f8f8f8}
.r23 {color: #0000ff; text-decoration-color: #0000ff}
body {
    color: #000000;
    background-color: #ffffff;
}
</style>
</head>
<html>
<body>
    <code>
        <pre style="font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">                                       Memory usage: <span class="r1">▄▅█</span> (max: 20.109 MB, growth rate: 100%)                                       
                          /dist/flax-fm/flaxfm/model/nfm.py: % of time =  99.76% (2.782s) out of 2.788s.                           
       ╷       ╷       ╷      ╷       ╷       ╷       ╷               ╷       ╷                                                    
 <span class="r2">      </span>│<span class="r3">Time</span><span class="r2">   </span>│<span class="r3">––––––</span><span class="r2"> </span>│<span class="r3">––––…</span><span class="r2"> </span>│<span class="r4">––––––</span><span class="r2"> </span>│<span class="r5">Memory</span><span class="r2"> </span>│<span class="r5">––––––</span><span class="r2"> </span>│<span class="r5">–––––––––––</span><span class="r2">    </span>│<span class="r4">Copy</span><span class="r2">   </span>│<span class="r2">                                                   </span> 
 <span class="r2"> </span><span class="r6">Line</span><span class="r2"> </span>│<span class="r7">Python</span><span class="r2"> </span>│<span class="r7">native</span><span class="r2"> </span>│<span class="r7">syst…</span><span class="r2"> </span>│<span class="r8">GPU</span><span class="r2">    </span>│<span class="r9">Python</span><span class="r2"> </span>│<span class="r9">peak</span><span class="r2">   </span>│<span class="r9">timeline</span><span class="r5">/%</span><span class="r2">     </span>│<span class="r8">(MB/s)</span><span class="r2"> </span>│<span class="r2">/dist/flax-fm/flaxfm/model/nfm.py                  </span> 
╺━━━━━━┿━━━━━━━┿━━━━━━━┿━━━━━━┿━━━━━━━┿━━━━━━━┿━━━━━━━┿━━━━━━━━━━━━━━━┿━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸
 <span class="r10">    1 </span>│<span class="r11">   31%</span><span class="r12"> </span>│<span class="r11">   64%</span><span class="r12"> </span>│<span class="r12">   4% </span>│<span class="r13">       </span>│<span class="r1">  99%  </span>│<span class="r1">   20M </span>│<span class="r11">▄█ 100%</span><span class="r1">        </span>│<span class="r13">   159 </span>│<span class="r14">from</span><span class="r15"> </span><span class="r16">flax</span><span class="r15"> </span><span class="r14">import</span><span class="r15"> linen </span><span class="r14">as</span><span class="r15"> nn</span><span class="r17">                      </span>  
 <span class="r10">    2 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r14">import</span><span class="r15"> </span><span class="r16">jax</span><span class="r17">                                        </span>  
 <span class="r10">    3 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r14">from</span><span class="r15"> </span><span class="r16">flaxfm.layer</span><span class="r15"> </span><span class="r14">import</span><span class="r15">  FactorizationMachineFlax</span>  
 <span class="r10">    4 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r14">import</span><span class="r15"> </span><span class="r16">numpy</span><span class="r15"> </span><span class="r14">as</span><span class="r15"> </span><span class="r16">np</span><span class="r17">                                </span>  
 <span class="r10">    5 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r14">from</span><span class="r15"> </span><span class="r16">typing</span><span class="r15"> </span><span class="r14">import</span><span class="r15"> Sequence</span><span class="r17">                       </span>  
 <span class="r10">    6 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r14">from</span><span class="r15"> </span><span class="r16">flaxfm.utils</span><span class="r15"> </span><span class="r14">import</span><span class="r15"> Sequential</span><span class="r17">               </span>  
 <span class="r10">    7 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r17">                                                  </span>  
 <span class="r10">    8 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r14">class</span><span class="r15"> </span><span class="r16">NeuralFactorizationMachineModelFlax</span><span class="r15">(nn</span><span class="r18">.</span><span class="r15">Modul</span>  
 <span class="r10">    9 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">    </span><span class="r19">&quot;&quot;&quot;</span><span class="r17">                                           </span>  
 <span class="r10">   10 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r19">    Note:</span><span class="r17">                                         </span>  
 <span class="r10">   11 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r19">    You can only assign Module attributes to self </span>  
 <span class="r10">   12 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r19">    Outside of that method, the Module instance is</span>  
 <span class="r10">   13 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r19">    This behavior is similar to frozen Python data</span>  
 <span class="r10">   14 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r19">    (https://flax.readthedocs.io/en/latest/api_ref</span>  
 <span class="r10">   15 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r19">    &quot;&quot;&quot;</span><span class="r17">                                           </span>  
 <span class="r10">   16 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">    field_dims : np</span><span class="r18">.</span><span class="r15">ndarray</span><span class="r17">                       </span>  
 <span class="r10">   17 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">    embed_dim : </span><span class="r20">int</span><span class="r17">                               </span>  
 <span class="r10">   18 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">    mlp_dims : Sequence[</span><span class="r20">int</span><span class="r15">]</span><span class="r17">                      </span>  
 <span class="r10">   19 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">    dropout : </span><span class="r20">float</span><span class="r15"> </span><span class="r18">=</span><span class="r15"> </span><span class="r18">0.2</span><span class="r17">                         </span>  
 <span class="r10">   20 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r17">                                                  </span>  
 <span class="r10">   21 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">    </span><span class="r14">def</span><span class="r15"> </span><span class="r21">setup</span><span class="r15">(</span><span class="r20">self</span><span class="r15">):</span><span class="r17">                              </span>  
 <span class="r10">   22 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">        </span><span class="r20">self</span><span class="r18">.</span><span class="r15">embedding </span><span class="r18">=</span><span class="r15"> FeaturesEmbeddingFlax(</span><span class="r20">sel</span>  
 <span class="r10">   23 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">        </span><span class="r20">self</span><span class="r18">.</span><span class="r15">linear </span><span class="r18">=</span><span class="r15"> FeaturesLinearFlax(</span><span class="r20">self</span><span class="r18">.</span><span class="r15">fiel</span>  
 <span class="r10">   24 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">        </span><span class="r20">self</span><span class="r18">.</span><span class="r15">mlp </span><span class="r18">=</span><span class="r15"> MultiLayerPerceptronFlax(</span><span class="r20">self</span><span class="r18">.</span><span class="r15">m</span>  
 <span class="r10">   25 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">        </span><span class="r20">self</span><span class="r18">.</span><span class="r15">fm </span><span class="r18">=</span><span class="r15"> FactorizationMachineFlax(reduce_</span>  
 <span class="r10">   26 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r17">                                                  </span>  
 <span class="r10">   27 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">    </span><span class="r22">@nn</span><span class="r18">.</span><span class="r15">compact</span><span class="r17">                                   </span>  
 <span class="r10">   28 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">    </span><span class="r14">def</span><span class="r15"> </span><span class="r21">__call__</span><span class="r15">(</span><span class="r20">self</span><span class="r15">, x, training:</span><span class="r20">bool</span><span class="r18">=</span><span class="r14">True</span><span class="r15">):</span><span class="r17">    </span>  
 <span class="r10">   29 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">        fm </span><span class="r18">=</span><span class="r15"> Sequential([</span><span class="r20">self</span><span class="r18">.</span><span class="r15">fm,</span><span class="r17">                 </span>  
 <span class="r10">   30 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">                            nn</span><span class="r18">.</span><span class="r15">BatchNorm(use_runni</span>  
 <span class="r10">   31 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">                            nn</span><span class="r18">.</span><span class="r15">Dropout(rate</span><span class="r18">=</span><span class="r20">self</span><span class="r18">.</span><span class="r15">d</span>  
 <span class="r10">   32 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">                            ])</span><span class="r17">                    </span>  
 <span class="r10">   33 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">        cross_term </span><span class="r18">=</span><span class="r15"> fm(</span><span class="r20">self</span><span class="r18">.</span><span class="r15">embedding(x))</span><span class="r17">        </span>  
 <span class="r10">   34 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">        x </span><span class="r18">=</span><span class="r15"> </span><span class="r20">self</span><span class="r18">.</span><span class="r15">linear(x) </span><span class="r18">+</span><span class="r15"> </span><span class="r20">self</span><span class="r18">.</span><span class="r15">mlp(cross_term, </span>  
 <span class="r10">   35 </span>│<span class="r12">       </span>│<span class="r12">       </span>│<span class="r12">      </span>│<span class="r13">       </span>│<span class="r1">       </span>│<span class="r1">       </span>│<span class="r1">               </span>│<span class="r13">       </span>│<span class="r15">        </span><span class="r14">return</span><span class="r15"> jax</span><span class="r18">.</span><span class="r15">nn</span><span class="r18">.</span><span class="r15">sigmoid(x</span><span class="r18">.</span><span class="r15">squeeze(</span><span class="r18">1</span><span class="r15">))</span><span class="r17">       </span>  
       ╵       ╵       ╵      ╵       ╵       ╵       ╵               ╵       ╵                                                    
Top PEAK memory consumption, by line:
<span class="r1">(1)     1:    20 MB</span>                                                                                                                 
generated by the <a class="r23" href="https://github.com/plasma-umass/scalene">scalene</a> profiler                                                                                                   
</pre>
    </code>
</body>
</html>
